{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travaillons la donnée pour constituer un fichier JSON de la forme : \n",
    "\n",
    "{  \n",
    "  \"id\": \"<UUID ou concat source+num>\",  \n",
    "  \"text\": \"<contenu du passage>\",  \n",
    "  \"metadata\": {  \n",
    "    \"source\": \"<nom de la CSV ou type de document>\",  \n",
    "    \"date\": \"YYYY-MM-DD\",  \n",
    "    \"section\": \"<titre de la section>\",  \n",
    "    // autres champs utiles…  \n",
    "  }  \n",
    "}\n",
    "\n",
    "Il faudra également chunker :\n",
    " \n",
    "Découpage en passages de ~500 mots (ou ~2 000 caractères) pour un meilleur score de similarité.\n",
    "\t•\tVous pouvez chunker par paragraphes puis regrouper si trop courts\n",
    "\t•\tConserver les métadonnées à chaque chunk (idem date, section…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "\n",
    "path = './fiscal_data_process_20232024/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Fonctions générales - Chunking & JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_chunk_dataframe(df: pd.DataFrame, column: str, chunk_size: int = 500) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimized version: utilise apply + explode pour découper la colonne textuelle en chunks\n",
    "    et dupliquer les autres colonnes de manière vectorisée.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame d'entrée.\n",
    "        column (str): Nom de la colonne à chunker.\n",
    "        chunk_size (int, optional): Taille maximale des chunks en caractères. Par défaut 500.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame transformé avec colonnes dupliquées et colonne chunkée.\n",
    "    \"\"\"\n",
    "    # 1. Créer une nouvelle colonne 'chunks' contenant la liste des morceaux\n",
    "    df = df.copy()\n",
    "    df['chunks'] = df[column].apply(lambda txt: textwrap.wrap(txt, width=chunk_size))\n",
    "    \n",
    "    # 2. Exploser cette colonne pour obtenir une ligne par chunk\n",
    "    df_expanded = df.explode('chunks').reset_index(drop=True)\n",
    "    \n",
    "    # 3. Remplacer l'ancienne colonne par les chunks et supprimer la colonne temporaire\n",
    "    df_expanded[column] = df_expanded['chunks']\n",
    "    return df_expanded.drop(columns=['chunks'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_to_jsonl(df: pd.DataFrame, id_col: str, text_col: str, metadata_cols: list, output_path: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame into a list of JSON Lines (JSONL) strings with the structure:\n",
    "    {\n",
    "      \"id\": \"<id value>\",\n",
    "      \"text\": \"<text value>\",\n",
    "      \"metadata\": {\n",
    "        \"<meta1>\": \"<value1>\",\n",
    "        \"<meta2>\": \"<value2>\",\n",
    "        ...\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        id_col (str): Name of the column to use as the document ID.\n",
    "        text_col (str): Name of the column containing the text content.\n",
    "        metadata_cols (list): List of column names to include in the metadata.\n",
    "        output_path (str, optional): If provided, writes JSONL to this file path.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of JSONL strings.\n",
    "    \"\"\"\n",
    "    jsonl_lines = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        record = {\n",
    "            \"id\": str(row[id_col]),\n",
    "            \"text\": row[text_col],\n",
    "            \"metadata\": {col: row[col] for col in metadata_cols}\n",
    "        }\n",
    "        jsonl_line = json.dumps(record, ensure_ascii=False)\n",
    "        jsonl_lines.append(jsonl_line)\n",
    "    \n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for line in jsonl_lines:\n",
    "                f.write(line + '\\n')\n",
    "    \n",
    "    return jsonl_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constitution du JSON general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOFIP\n",
    "df_bofip = pd.read_csv(path + 'Bofip_2023_2024.csv', sep =\";\")\n",
    "\n",
    "df_bofip_chunked = optimized_chunk_dataframe(df_bofip, 'contenu', chunk_size=500)\n",
    "json_bofip = df_to_jsonl(df_bofip_chunked, id_col='identifiant_juridique', text_col='contenu',\n",
    "                        metadata_cols=['division', 'debut_de_validite', 'serie', 'titre'],\n",
    "                        output_path='./json_chunk/bofip.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CGI\n",
    "df_cgi = pd.read_csv(path + 'CGI_2023_2024.csv', sep =\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cgi_chunked = optimized_chunk_dataframe(df_cgi, 'content', chunk_size=500)\n",
    "json_cgi = df_to_jsonl(df_cgi_chunked, id_col='id', text_col='content',\n",
    "                        metadata_cols=['num', 'etat',  'pathTitle','cid', 'section_title', 'parent_title'],\n",
    "                        output_path='./json_chunk/cgi.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Jurisprudence\n",
    "df_juris = pd.read_csv(path + 'Juris_filter_2023_2024.csv', sep =\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_juris_chunked = optimized_chunk_dataframe(df_juris, 'texte_integral', chunk_size=500)\n",
    "json_juris = df_to_jsonl(df_juris_chunked, id_col='id', text_col='texte_integral',\n",
    "                        metadata_cols=['date_maj', 'code_juridiction', 'juridiction','date_decision',  'type_decision', 'type_recours',\n",
    "                                          'solution', 'date_audience', 'formation_jugement'],\n",
    "                        output_path='./json_chunk/juris.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LPF\n",
    "df_lpf = pd.read_csv(path + 'LPF_2023_2024.csv', sep =\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lpf_chunked = optimized_chunk_dataframe(df_lpf, 'content', chunk_size=500)\n",
    "json_lpf = df_to_jsonl(df_lpf_chunked, id_col='id', text_col='content',\n",
    "                        metadata_cols=[ 'num', 'etat', 'pathTitle', 'section_title', 'parent_title'],\n",
    "                        output_path='./json_chunk/lpf.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QA AN\n",
    "df_qa = pd.read_csv(path + 'QA_AN_2023_2024.csv', sep =\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa['QA'] = \" Question : \" + df_qa['texte_question'] + \"\\n Reponse : \" + df_qa['texte_reponse'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_chunked = optimized_chunk_dataframe(df_qa, 'QA', chunk_size=500)\n",
    "json_qa = df_to_jsonl(df_qa_chunked, id_col='uid', text_col='QA',\n",
    "                        metadata_cols=['legislature', 'rubrique', 'analyse','auteur_groupe', 'ministere', 'date_question', 'date_reponse', 'cloture_date','date_publication'],\n",
    "                        output_path='./json_chunk/qa.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL\n",
    "def merge_jsonl_lists(lists_of_lines: list, output_path: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Fusionne plusieurs listes de lignes JSONL en une seule liste.\n",
    "\n",
    "    Args:\n",
    "        lists_of_lines (list of list of str): Listes de chaînes JSONL.\n",
    "        output_path (str, optional): Si fourni, écrit la liste fusionnée dans ce fichier.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Liste fusionnée de lignes JSONL.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for lines in lists_of_lines:\n",
    "        merged.extend(lines)\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for line in merged:\n",
    "                # S’assure qu’il y a un seul passage à la ligne par enregistrement\n",
    "                f.write(line.rstrip('\\n') + '\\n')\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = merge_jsonl_lists([json_bofip, json_cgi, json_juris, json_lpf, json_qa], output_path='./json_chunk/all_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778750"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778750"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
