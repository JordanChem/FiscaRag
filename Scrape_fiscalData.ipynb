{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Paramètres d'authentification PISTE\n",
    "CLIENT_ID = \"980d9205-5180-43d4-bf91-b2190855e925\"  \n",
    "CLIENT_SECRET = \"18775315-8a1f-48bb-97d0-8a1c708352fc\" \n",
    "\n",
    "\n",
    "oauth_url = \" https://oauth.piste.gouv.fr/api/oauth/token\"\n",
    "api_url = \"https://api.piste.gouv.fr/dila/legifrance/lf-engine-app/consult/legiPart\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_access_token():\n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": CLIENT_ID,\n",
    "        \"client_secret\": CLIENT_SECRET,\n",
    "        \"scope\": \"openid\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(oauth_url, data=data)\n",
    "    response.raise_for_status()\n",
    "    token = response.json()[\"access_token\"]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_timestamp_ms(date_str):\n",
    "    dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    timestamp_ms = int(dt.timestamp()) * 1000\n",
    "    return timestamp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Code général des impôts  (2023-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cgi_version(token, timestamp_ms, year):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"date\": timestamp_ms,\n",
    "        \"textId\": \"LEGITEXT000006069577\"  # Code général des impôts\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Convertir la réponse en DataFrame\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data['sections'])\n",
    "    \n",
    "    # Sauvegarder en CSV\n",
    "    csv_path = f\"\"\"./fiscal_data/cgi_{year}.csv\"\"\"\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Version {year} sauvegardée avec succès dans {csv_path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 2023 sauvegardée avec succès dans ./fiscal_data/cgi_2023.csv\n",
      "Version 2024 sauvegardée avec succès dans ./fiscal_data/cgi_2024.csv\n"
     ]
    }
   ],
   "source": [
    "token = get_access_token()\n",
    "\n",
    "dates = {\n",
    "    \"2023\": date_to_timestamp_ms(\"2023-01-01\"),\n",
    "    \"2024\": date_to_timestamp_ms(\"2024-01-01\")\n",
    "}\n",
    "\n",
    "cgi_2023 = get_cgi_version(token, dates[\"2023\"],\"2023\")\n",
    "cgi_2024 = get_cgi_version(token, dates[\"2024\"],\"2024\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Livre des procédures fiscales (LPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lpf_version(token, timestamp_ms, year):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"date\": timestamp_ms,\n",
    "        \"textId\": \"LEGITEXT000006069583\"  # Livre des procédures fiscales\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Convertir la réponse en DataFrame\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data['sections'])\n",
    "\n",
    "    # Sauvegarder en CSV\n",
    "    csv_path = f\"\"\"./fiscal_data/lpf_{year}.csv\"\"\"\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"LPF - Version {year} sauvegardée avec succès dans {csv_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPF - Version 2023 sauvegardée avec succès dans ./fiscal_data/lpf_2023.csv\n",
      "LPF - Version 2024 sauvegardée avec succès dans ./fiscal_data/lpf_2024.csv\n"
     ]
    }
   ],
   "source": [
    "token = get_access_token()\n",
    "\n",
    "dates = {\n",
    "    \"2023\": date_to_timestamp_ms(\"2023-01-01\"),\n",
    "    \"2024\": date_to_timestamp_ms(\"2024-01-01\")\n",
    "}\n",
    "\n",
    "lpf_2023 = get_lpf_version(token, dates[\"2023\"],\"2023\")\n",
    "lpf_2024 = get_lpf_version(token, dates[\"2024\"],\"2024\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) BOFIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_bofip() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Récupère l'intégralité du dataset BOFiP-vigueur.\n",
    "    \"\"\"\n",
    "    url = \"https://data.economie.gouv.fr/api/records/1.0/search/\"\n",
    "    dataset = \"bofip-vigueur\"\n",
    "    rows_per_page = 1000\n",
    "    start = 0\n",
    "    all_records = []\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"rows\": rows_per_page,\n",
    "            \"start\": start,\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        records = data.get(\"records\", [])\n",
    "        \n",
    "        if not records:\n",
    "            break\n",
    "        \n",
    "        all_records.extend([record[\"fields\"] for record in records])\n",
    "        start += rows_per_page\n",
    "        print(f\"Fetched {len(all_records)} records so far...\")\n",
    "    \n",
    "    df = pd.DataFrame(all_records)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 1000 records so far...\n",
      "Fetched 2000 records so far...\n",
      "Fetched 3000 records so far...\n",
      "Fetched 4000 records so far...\n",
      "Fetched 5000 records so far...\n",
      "Fetched 6000 records so far...\n",
      "Fetched 7000 records so far...\n",
      "Fetched 8000 records so far...\n",
      "Fetched 8787 records so far...\n"
     ]
    }
   ],
   "source": [
    "# Récupération complète\n",
    "df_all = fetch_full_bofip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f\"\"\"./fiscal_data/bofip.csv\"\"\"\n",
    "df_all.to_csv(csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV) Jurisprudence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "juridictions = [\n",
    "    \"Conseil d'État\",\n",
    "    \"Cour administrative d'appel\",\n",
    "    \"Tribunal administratif\",\n",
    "    \"Cour de cassation\",\n",
    "    \"Cour d'appel\",\n",
    "    \"Tribunal judiciaire\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On contourne en les téléchargeant manuellement et en mettant les zip dans le dossier Jurisprudence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du fichier : ./Jurisprudence/CE_202408.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202409.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202309.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202408.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202409.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202308.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202409.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202308.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202309.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202408.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202308.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202309.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202304.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202310.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202301.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202311.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202305.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202401.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202403.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202302.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202307.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202306.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202303.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202312.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202402.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202412.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202406.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202307.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202302.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202303.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202312.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202306.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202407.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202405.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202411.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202310.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202301.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202304.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202305.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202311.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202410.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202404.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202305.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202311.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202410.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202401.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202404.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202405.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202411.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202310.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202304.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202312.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202306.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202407.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202402.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202403.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202412.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202406.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202307.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202303.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202402.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202407.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202406.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202403.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202412.zip\n",
      "Erreur sur le fichier DCE_487157_20241218.xml: Document is empty, line 1, column 1 (DCE_487157_20241218.xml, line 1)\n",
      "Erreur sur le fichier DCE_491216_20241216.xml: Document is empty, line 1, column 1 (DCE_491216_20241216.xml, line 1)\n",
      "Erreur sur le fichier DCE_493217_20241218.xml: Document is empty, line 1, column 1 (DCE_493217_20241218.xml, line 1)\n",
      "Erreur sur le fichier DCE_495565_20241218.xml: Document is empty, line 1, column 1 (DCE_495565_20241218.xml, line 1)\n",
      "Erreur sur le fichier DCE_499276_20241216.xml: Document is empty, line 1, column 1 (DCE_499276_20241216.xml, line 1)\n",
      "Erreur sur le fichier DCE_499411_20241218.xml: Document is empty, line 1, column 1 (DCE_499411_20241218.xml, line 1)\n",
      "Erreur sur le fichier DCE_499611_20241218.xml: Document is empty, line 1, column 1 (DCE_499611_20241218.xml, line 1)\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202302.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202404.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202410.zip\n",
      "Traitement du fichier : ./Jurisprudence/TA_202401.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202411.zip\n",
      "Traitement du fichier : ./Jurisprudence/CE_202405.zip\n",
      "Traitement du fichier : ./Jurisprudence/CAA_202301.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "# Dossier racine contenant tous les fichiers zip\n",
    "ROOT_FOLDER = './Jurisprudence'\n",
    "OUTPUT_CSV = './fiscal_data/jurisprudence_global.csv'\n",
    "\n",
    "# Fonction de parsing d'un fichier XML unique\n",
    "def parse_xml(file):\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "    tree = etree.parse(file, parser)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    id_doc = root.findtext('Donnees_Techniques/Identification')\n",
    "    date_maj = root.findtext('Donnees_Techniques/Date_Mise_Jour')\n",
    "    code_juridiction = root.findtext('Dossier/Code_Juridiction')\n",
    "    juridiction = root.findtext('Dossier/Nom_Juridiction')\n",
    "    numero_dossier = root.findtext('Dossier/Numero_Dossier')\n",
    "    date_lecture = root.findtext('Dossier/Date_Lecture')\n",
    "    avocat_requerant = root.findtext('Dossier/Avocat_Requerant')\n",
    "    type_decision = root.findtext('Dossier/Type_Decision')\n",
    "    type_recours = root.findtext('Dossier/Type_Recours')\n",
    "    code_publication = root.findtext('Dossier/Code_Publication')\n",
    "    solution = root.findtext('Dossier/Solution')\n",
    "    date_audience = root.findtext('Audience/Date_Audience')\n",
    "    numero_role = root.findtext('Audience/Numero_Role')\n",
    "    formation_jugement = root.findtext('Audience/Formation_Jugement')\n",
    "\n",
    "    # Pour le texte intégral, on va récupérer tout le contenu XML sous forme brute\n",
    "    texte_integral_elem = root.find('Decision/Texte_Integral')\n",
    "    texte_integral = ''\n",
    "    if texte_integral_elem is not None:\n",
    "        texte_integral = etree.tostring(texte_integral_elem, method=\"text\", encoding=\"unicode\").strip()\n",
    "\n",
    "    return {\n",
    "        \"id\": id_doc,\n",
    "        \"date_maj\": date_maj,\n",
    "        \"code_juridiction\": code_juridiction,\n",
    "        \"juridiction\": juridiction,\n",
    "        \"numero_dossier\": numero_dossier,\n",
    "        \"date_decision\": date_lecture,\n",
    "        \"avocat_requerant\": avocat_requerant,\n",
    "        \"type_decision\": type_decision,\n",
    "        \"type_recours\": type_recours,\n",
    "        \"code_publication\": code_publication,\n",
    "        \"solution\": solution,\n",
    "        \"date_audience\": date_audience,\n",
    "        \"numero_role\": numero_role,\n",
    "        \"formation_jugement\": formation_jugement,\n",
    "        \"texte_integral\": texte_integral\n",
    "    }\n",
    "\n",
    "# Préparer le fichier de sortie (création des colonnes au début)\n",
    "columns = [\"id\", \"date_maj\", \"code_juridiction\", \"juridiction\", \"numero_dossier\", \n",
    "          \"date_decision\", \"avocat_requerant\", \"type_decision\", \"type_recours\",\n",
    "          \"code_publication\", \"solution\", \"date_audience\", \"numero_role\",\n",
    "          \"formation_jugement\", \"texte_integral\"]\n",
    "pd.DataFrame(columns=columns).to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "# Parcours de tous les fichiers zip dans le dossier\n",
    "for root, dirs, files in os.walk(ROOT_FOLDER):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.zip'):\n",
    "            zip_path = os.path.join(root, filename)\n",
    "            print(f\"Traitement du fichier : {zip_path}\")\n",
    "\n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                for file_in_zip in z.namelist():\n",
    "                    if file_in_zip.endswith('.xml'):\n",
    "                        try:\n",
    "                            with z.open(file_in_zip) as f:\n",
    "                                record = parse_xml(f)\n",
    "                                pd.DataFrame([record]).to_csv(OUTPUT_CSV, mode='a', header=False, index=False, encoding='utf-8')\n",
    "                        except Exception as e:\n",
    "                            print(f\"Erreur sur le fichier {file_in_zip}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/_m5cx42n4bzg7rgn2xc3p0s80000gn/T/ipykernel_13075/2971875856.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_juri = pd.read_csv('./fiscal_data/jurisprudence_global.csv')\n"
     ]
    }
   ],
   "source": [
    "df_juri = pd.read_csv('./fiscal_data/jurisprudence_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_maj</th>\n",
       "      <th>code_juridiction</th>\n",
       "      <th>juridiction</th>\n",
       "      <th>numero_dossier</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>avocat_requerant</th>\n",
       "      <th>type_decision</th>\n",
       "      <th>type_recours</th>\n",
       "      <th>code_publication</th>\n",
       "      <th>solution</th>\n",
       "      <th>date_audience</th>\n",
       "      <th>numero_role</th>\n",
       "      <th>formation_jugement</th>\n",
       "      <th>texte_integral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DCE_470759_20240827.xml</td>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>CE</td>\n",
       "      <td>Section du Contentieux</td>\n",
       "      <td>470759</td>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>OCCHIPINTI</td>\n",
       "      <td>Décision</td>\n",
       "      <td>Plein contentieux</td>\n",
       "      <td>C</td>\n",
       "      <td>Renvoi après cassation</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>24412</td>\n",
       "      <td>4ème chambre jugeant seule</td>\n",
       "      <td>Vu la procédure suivante :\\nPar une décision d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DCE_471080_20240827.xml</td>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>CE</td>\n",
       "      <td>Section du Contentieux</td>\n",
       "      <td>471080</td>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Décision</td>\n",
       "      <td>Excès de pouvoir</td>\n",
       "      <td>C</td>\n",
       "      <td>Rejet</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>24412</td>\n",
       "      <td>4ème chambre jugeant seule</td>\n",
       "      <td>Vu la procédure suivante :\\n1° Sous le n° 4710...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCE_475455_20240827.xml</td>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>CE</td>\n",
       "      <td>Section du Contentieux</td>\n",
       "      <td>475455</td>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>SCP RICHARD</td>\n",
       "      <td>Décision</td>\n",
       "      <td>Plein contentieux</td>\n",
       "      <td>C</td>\n",
       "      <td>Renvoi après cassation</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>24412</td>\n",
       "      <td>4ème chambre jugeant seule</td>\n",
       "      <td>Vu la procédure suivante :\\nMme D C a porté pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DCE_485331_20240827.xml</td>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>CE</td>\n",
       "      <td>Section du Contentieux</td>\n",
       "      <td>485331</td>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>SARL MATUCHANSKY, POUPOT, VALDELIEVRE, RAMEIX</td>\n",
       "      <td>Décision</td>\n",
       "      <td>Plein contentieux</td>\n",
       "      <td>C</td>\n",
       "      <td>Renvoi après cassation</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>24412</td>\n",
       "      <td>4ème chambre jugeant seule</td>\n",
       "      <td>Vu la procédure suivante :\\nMme D C a porté pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DCE_488162_20240827.xml</td>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>CE</td>\n",
       "      <td>Section du Contentieux</td>\n",
       "      <td>488162</td>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>SCP ROCHETEAU, UZAN-SARANO &amp; GOULET</td>\n",
       "      <td>Décision</td>\n",
       "      <td>Plein contentieux</td>\n",
       "      <td>C</td>\n",
       "      <td>Renvoi après cassation</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>24412</td>\n",
       "      <td>4ème chambre jugeant seule</td>\n",
       "      <td>Vu la procédure suivante :\\nLe directeur de l'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id    date_maj code_juridiction  \\\n",
       "0  DCE_470759_20240827.xml  2024-08-29               CE   \n",
       "1  DCE_471080_20240827.xml  2024-08-29               CE   \n",
       "2  DCE_475455_20240827.xml  2024-08-29               CE   \n",
       "3  DCE_485331_20240827.xml  2024-08-29               CE   \n",
       "4  DCE_488162_20240827.xml  2024-08-29               CE   \n",
       "\n",
       "               juridiction numero_dossier date_decision  \\\n",
       "0  Section du Contentieux          470759    2024-08-27   \n",
       "1  Section du Contentieux          471080    2024-08-27   \n",
       "2  Section du Contentieux          475455    2024-08-27   \n",
       "3  Section du Contentieux          485331    2024-08-27   \n",
       "4  Section du Contentieux          488162    2024-08-27   \n",
       "\n",
       "                                avocat_requerant type_decision  \\\n",
       "0                                     OCCHIPINTI      Décision   \n",
       "1                                            NaN      Décision   \n",
       "2                                    SCP RICHARD      Décision   \n",
       "3  SARL MATUCHANSKY, POUPOT, VALDELIEVRE, RAMEIX      Décision   \n",
       "4            SCP ROCHETEAU, UZAN-SARANO & GOULET      Décision   \n",
       "\n",
       "        type_recours code_publication                 solution date_audience  \\\n",
       "0  Plein contentieux                C  Renvoi après cassation     2024-07-05   \n",
       "1   Excès de pouvoir                C                    Rejet    2024-07-05   \n",
       "2  Plein contentieux                C  Renvoi après cassation     2024-07-05   \n",
       "3  Plein contentieux                C  Renvoi après cassation     2024-07-05   \n",
       "4  Plein contentieux                C  Renvoi après cassation     2024-07-05   \n",
       "\n",
       "  numero_role           formation_jugement  \\\n",
       "0       24412   4ème chambre jugeant seule   \n",
       "1       24412   4ème chambre jugeant seule   \n",
       "2       24412   4ème chambre jugeant seule   \n",
       "3       24412   4ème chambre jugeant seule   \n",
       "4       24412   4ème chambre jugeant seule   \n",
       "\n",
       "                                      texte_integral  \n",
       "0  Vu la procédure suivante :\\nPar une décision d...  \n",
       "1  Vu la procédure suivante :\\n1° Sous le n° 4710...  \n",
       "2  Vu la procédure suivante :\\nMme D C a porté pl...  \n",
       "3  Vu la procédure suivante :\\nMme D C a porté pl...  \n",
       "4  Vu la procédure suivante :\\nLe directeur de l'...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_juri.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V) QA assemblée nationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def download_taz_files(year: int, output_dir: str = \"./QA_AN\") -> list:\n",
    "    \"\"\"\n",
    "    Télécharge les fichiers .taz pour une année donnée\n",
    "    \n",
    "    Args:\n",
    "        year (int): Année à télécharger\n",
    "        output_dir (str): Dossier de sortie pour les fichiers .taz\n",
    "        \n",
    "    Returns:\n",
    "        list: Liste des chemins des fichiers .taz téléchargés\n",
    "    \"\"\"\n",
    "    # Créer le dossier de sortie s'il n'existe pas\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    base_url = f\"https://echanges.dila.gouv.fr/OPENDATA/Questions-Reponses/AN/{year}/\"\n",
    "    \n",
    "    # Récupérer la liste des fichiers disponibles\n",
    "    index_page = requests.get(base_url)\n",
    "    index_page.raise_for_status()\n",
    "    \n",
    "    # Trouver les fichiers .taz\n",
    "    files = sorted(set(re.findall(r'ANQ\\d{8}\\.taz', index_page.text)))\n",
    "    print(f\"✅ {len(files)} fichiers trouvés pour l'année {year}\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    # Télécharger chaque fichier\n",
    "    for fichier in files:\n",
    "        url = base_url + fichier\n",
    "        taz_path = os.path.join(output_dir, fichier)\n",
    "        \n",
    "        # Vérifier si le fichier existe déjà\n",
    "        if os.path.exists(taz_path):\n",
    "            print(f\"Le fichier {fichier} existe déjà, on passe au suivant\")\n",
    "            downloaded_files.append(taz_path)\n",
    "            continue\n",
    "            \n",
    "        print(f\"Téléchargement de {fichier}...\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Sauvegarder le fichier .taz\n",
    "            with open(taz_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            downloaded_files.append(taz_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors du téléchargement de {fichier}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "def extract_and_parse_xml(taz_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrait et parse les fichiers XML d'un fichier .taz\n",
    "    \n",
    "    Args:\n",
    "        taz_file (str): Chemin vers le fichier .taz\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contenant les données extraites\n",
    "    \"\"\"\n",
    "    all_questions = []\n",
    "    \n",
    "    try:\n",
    "        with tarfile.open(taz_file, 'r') as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and member.name.endswith(\".xml\"):\n",
    "                    xml_file = tar.extractfile(member)\n",
    "                    parser = etree.XMLParser(recover=True)\n",
    "                    tree = etree.parse(xml_file, parser)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    # Extraire les métadonnées du JO\n",
    "                    numero_jo = root.findtext('numero_jo')\n",
    "                    date_publication = root.findtext('date_publication')\n",
    "                    \n",
    "                    # Parcourir les questions/réponses\n",
    "                    for qr in root.findall('.//questionReponse'):\n",
    "                        questions = qr.findall('question')\n",
    "                        for q in questions:\n",
    "                            question_num = q.findtext('numero')\n",
    "                            question_page = q.findtext('page_jo')\n",
    "                            \n",
    "                            reponse = qr.find('reponse')\n",
    "                            if reponse is not None:\n",
    "                                reponse_page = reponse.findtext('page_jo')\n",
    "                                reponse_texte = reponse.findtext('texteReponse')\n",
    "                            else:\n",
    "                                reponse_page = None\n",
    "                                reponse_texte = None\n",
    "                            \n",
    "                            all_questions.append({\n",
    "                                'numero_jo': numero_jo,\n",
    "                                'date_publication': date_publication,\n",
    "                                'question_numero': question_num,\n",
    "                                'question_page': question_page,\n",
    "                                'reponse_page': reponse_page,\n",
    "                                'reponse_texte': reponse_texte\n",
    "                            })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du traitement de {taz_file}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(all_questions)\n",
    "\n",
    "def process_year(year: int, output_dir: str = \"./QA_AN\", fiscal_data_dir: str = \"./fiscal_data\"):\n",
    "    \"\"\"\n",
    "    Traite une année complète : télécharge les fichiers .taz, extrait les données\n",
    "    et crée un fichier CSV final\n",
    "    \n",
    "    Args:\n",
    "        year (int): Année à traiter\n",
    "        output_dir (str): Dossier pour les fichiers .taz\n",
    "        fiscal_data_dir (str): Dossier pour le fichier CSV final\n",
    "    \"\"\"\n",
    "    # Créer les dossiers nécessaires\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(fiscal_data_dir, exist_ok=True)\n",
    "    \n",
    "    # Télécharger les fichiers .taz\n",
    "    taz_files = download_taz_files(year, output_dir)\n",
    "    \n",
    "    # Traiter chaque fichier .taz\n",
    "    all_data = []\n",
    "    for taz_file in taz_files:\n",
    "        print(f\"Traitement de {os.path.basename(taz_file)}...\")\n",
    "        df = extract_and_parse_xml(taz_file)\n",
    "        if not df.empty:\n",
    "            all_data.append(df)\n",
    "    \n",
    "    # Combiner tous les DataFrames\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Sauvegarder le fichier CSV final\n",
    "        output_file = os.path.join(fiscal_data_dir, f\"questions_reponses_AN_{year}.csv\")\n",
    "        final_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"✅ Fichier CSV créé : {output_file}\")\n",
    "        print(f\"✅ {len(final_df)} questions traitées\")\n",
    "    else:\n",
    "        print(\"❌ Aucune donnée n'a été extraite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 38 fichiers trouvés pour l'année 2024\n",
      "Le fichier ANQ20240001.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240002.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240003.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240004.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240005.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240006.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240007.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240008.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240009.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240010.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240011.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240012.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240013.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240014.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240015.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240016.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240017.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240018.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240019.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240020.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240021.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240022.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240023.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240024.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240025.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240026.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240027.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240028.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240029.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240030.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240031.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240032.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240033.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240034.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240035.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240036.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240037.taz existe déjà, on passe au suivant\n",
      "Le fichier ANQ20240038.taz existe déjà, on passe au suivant\n",
      "Traitement de ANQ20240001.taz...\n",
      "Traitement de ANQ20240002.taz...\n",
      "Traitement de ANQ20240003.taz...\n",
      "Traitement de ANQ20240004.taz...\n",
      "Traitement de ANQ20240005.taz...\n",
      "Traitement de ANQ20240006.taz...\n",
      "Traitement de ANQ20240007.taz...\n",
      "Traitement de ANQ20240008.taz...\n",
      "Traitement de ANQ20240009.taz...\n",
      "Traitement de ANQ20240010.taz...\n",
      "Traitement de ANQ20240011.taz...\n",
      "Traitement de ANQ20240012.taz...\n",
      "Traitement de ANQ20240013.taz...\n",
      "Traitement de ANQ20240014.taz...\n",
      "Traitement de ANQ20240015.taz...\n",
      "Traitement de ANQ20240016.taz...\n",
      "Traitement de ANQ20240017.taz...\n",
      "Traitement de ANQ20240018.taz...\n",
      "Traitement de ANQ20240019.taz...\n",
      "Traitement de ANQ20240020.taz...\n",
      "Traitement de ANQ20240021.taz...\n",
      "Traitement de ANQ20240022.taz...\n",
      "Traitement de ANQ20240023.taz...\n",
      "Traitement de ANQ20240024.taz...\n",
      "Traitement de ANQ20240025.taz...\n",
      "Traitement de ANQ20240026.taz...\n",
      "Traitement de ANQ20240027.taz...\n",
      "Traitement de ANQ20240028.taz...\n",
      "Traitement de ANQ20240029.taz...\n",
      "Traitement de ANQ20240030.taz...\n",
      "Traitement de ANQ20240031.taz...\n",
      "Traitement de ANQ20240032.taz...\n",
      "Traitement de ANQ20240033.taz...\n",
      "Traitement de ANQ20240034.taz...\n",
      "Traitement de ANQ20240035.taz...\n",
      "Traitement de ANQ20240036.taz...\n",
      "Traitement de ANQ20240037.taz...\n",
      "Traitement de ANQ20240038.taz...\n",
      "✅ Fichier CSV créé : ./fiscal_data/questions_reponses_AN_2024.csv\n",
      "✅ 3179 questions traitées\n"
     ]
    }
   ],
   "source": [
    "process_year(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI) Senat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_questions_reponses_senat_sql():\n",
    "    # Télécharger le fichier zip depuis data.gouv\n",
    "    url = \"https://www.data.gouv.fr/en/datasets/r/407fcc2b-9c31-4f3d-b22b-c9b30cb69108\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Lire le zip en mémoire\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        sql_filename = [name for name in z.namelist() if name.endswith('.sql')][0]\n",
    "        print(f\"Lecture du fichier SQL : {sql_filename}\")\n",
    "\n",
    "        with z.open(sql_filename) as f:\n",
    "            sql_content = f.read().decode(\"ISO-8859-15\")\n",
    "\n",
    "    # Extraire les lignes INSERT INTO\n",
    "    insert_lines = re.findall(r\"INSERT INTO .*? VALUES\\s*(.*);\", sql_content, flags=re.DOTALL)\n",
    "\n",
    "    # On récupère toutes les valeurs\n",
    "    rows = []\n",
    "    for block in insert_lines:\n",
    "        # On sépare les tuples\n",
    "        tuples = re.findall(r\"\\((.*?)\\)\", block)\n",
    "        for tup in tuples:\n",
    "            # Séparer les champs individuellement\n",
    "            fields = []\n",
    "            splitted = re.split(r\",(?![^']*'\\s*,)\", tup)  # gère les virgules dans les valeurs texte\n",
    "            for field in splitted:\n",
    "                # Nettoyage des quotes SQL\n",
    "                field = field.strip()\n",
    "                if field == 'NULL':\n",
    "                    fields.append(None)\n",
    "                else:\n",
    "                    fields.append(ast.literal_eval(field))\n",
    "            rows.append(fields)\n",
    "\n",
    "    # Colonnes présentes dans le dump Sénat\n",
    "    columns = [\n",
    "        'id',\n",
    "        'numero_question',\n",
    "        'date_question',\n",
    "        'intitule_ministere',\n",
    "        'auteur',\n",
    "        'groupe',\n",
    "        'titre',\n",
    "        'url_question',\n",
    "        'texte_question',\n",
    "        'url_reponse',\n",
    "        'texte_reponse',\n",
    "        'date_reponse'\n",
    "    ]\n",
    "\n",
    "    # Création du DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Export CSV\n",
    "    df.to_csv(\"questions_reponses_senat.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Extraction SQL Sénat terminée avec {len(df)} questions\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mextract_questions_reponses_senat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mextract_questions_reponses_senat\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m response.raise_for_status()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Attention : encodage ISO-8859-15\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mISO-8859-15\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m df.to_csv(\u001b[33m\"\u001b[39m\u001b[33mquestions_reponses_senat.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Extraction Sénat terminée avec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m questions\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Bureau - MacBook Pro de Jordan/Jobs/Sociétés/Melundo Data/Missions/Tindle/Tindle_codebase/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Bureau - MacBook Pro de Jordan/Jobs/Sociétés/Melundo Data/Missions/Tindle/Tindle_codebase/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Bureau - MacBook Pro de Jordan/Jobs/Sociétés/Melundo Data/Missions/Tindle/Tindle_codebase/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Bureau - MacBook Pro de Jordan/Jobs/Sociétés/Melundo Data/Missions/Tindle/Tindle_codebase/.venv/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n"
     ]
    }
   ],
   "source": [
    "extract_questions_reponses_senat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
